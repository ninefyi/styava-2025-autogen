{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Literature Review\n",
                "\n",
                "A common task while exploring a new topic is to conduct a literature review. In this example we will explore how a multi-agent team can be configured to conduct a _simple_ literature review.\n",
                "\n",
                "- **Arxiv Search Agent**: Use the Arxiv API to search for papers related to a given topic and return results.\n",
                "- **Google Search Agent**: Use the Google Search api to find papers related to a given topic and return results.\n",
                "- **Report Agent**: Generate a report based on the information collected by the arxviv search and Google search agents.\n",
                "\n",
                "\n",
                "First, let us import the necessary modules. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from autogen_agentchat.agents import AssistantAgent\n",
                "from autogen_agentchat.conditions import TextMentionTermination\n",
                "from autogen_agentchat.teams import RoundRobinGroupChat\n",
                "from autogen_agentchat.ui import Console\n",
                "from autogen_core.tools import FunctionTool\n",
                "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Defining Tools \n",
                "\n",
                "Next, we will define the tools that the agents will use to perform their tasks. In this case we will define a simple function `search_arxiv` that will use the `arxiv` library to search for papers related to a given topic.  \n",
                "\n",
                "Finally, we will wrap the functions into a `FunctionTool` class that will allow us to use it as a tool in the agents. \n",
                "\n",
                "Note: You will need to set the appropriate environment variables for tools as needed.\n",
                "\n",
                "Also install required libraries: \n",
                "\n",
                "```bash\n",
                "!pip install arxiv\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:  # type: ignore[type-arg]\n",
                "    import os\n",
                "    import time\n",
                "\n",
                "    import requests\n",
                "    from bs4 import BeautifulSoup\n",
                "    from dotenv import load_dotenv\n",
                "\n",
                "    load_dotenv()\n",
                "\n",
                "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
                "    search_engine_id = os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\")\n",
                "\n",
                "    if not api_key or not search_engine_id:\n",
                "        raise ValueError(\"API key or Search Engine ID not found in environment variables\")\n",
                "\n",
                "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
                "    params = {\"key\": api_key, \"cx\": search_engine_id, \"q\": query, \"num\": num_results}\n",
                "\n",
                "    response = requests.get(url, params=params)  # type: ignore[arg-type]\n",
                "\n",
                "    if response.status_code != 200:\n",
                "        print(response.json())\n",
                "        raise Exception(f\"Error in API request: {response.status_code}\")\n",
                "\n",
                "    results = response.json().get(\"items\", [])\n",
                "\n",
                "    def get_page_content(url: str) -> str:\n",
                "        try:\n",
                "            response = requests.get(url, timeout=10)\n",
                "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
                "            text = soup.get_text(separator=\" \", strip=True)\n",
                "            words = text.split()\n",
                "            content = \"\"\n",
                "            for word in words:\n",
                "                if len(content) + len(word) + 1 > max_chars:\n",
                "                    break\n",
                "                content += \" \" + word\n",
                "            return content.strip()\n",
                "        except Exception as e:\n",
                "            print(f\"Error fetching {url}: {str(e)}\")\n",
                "            return \"\"\n",
                "\n",
                "    enriched_results = []\n",
                "    for item in results:\n",
                "        body = get_page_content(item[\"link\"])\n",
                "        enriched_results.append(\n",
                "            {\"title\": item[\"title\"], \"link\": item[\"link\"], \"snippet\": item[\"snippet\"], \"body\": body}\n",
                "        )\n",
                "        time.sleep(1)  # Be respectful to the servers\n",
                "\n",
                "    return enriched_results\n",
                "\n",
                "\n",
                "def arxiv_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\n",
                "    \"\"\"\n",
                "    Search Arxiv for papers and return the results including abstracts.\n",
                "    \"\"\"\n",
                "    import arxiv\n",
                "\n",
                "    client = arxiv.Client()\n",
                "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
                "\n",
                "    results = []\n",
                "    for paper in client.results(search):\n",
                "        results.append(\n",
                "            {\n",
                "                \"title\": paper.title,\n",
                "                \"authors\": [author.name for author in paper.authors],\n",
                "                \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n",
                "                \"abstract\": paper.summary,\n",
                "                \"pdf_url\": paper.pdf_url,\n",
                "            }\n",
                "        )\n",
                "\n",
                "    # # Write results to a file\n",
                "    # with open('arxiv_search_results.json', 'w') as f:\n",
                "    #     json.dump(results, f, indent=2)\n",
                "\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "google_search_tool = FunctionTool(\n",
                "    google_search, description=\"Search Google for information, returns results with a snippet and body content\"\n",
                ")\n",
                "arxiv_search_tool = FunctionTool(\n",
                "    arxiv_search, description=\"Search Arxiv for papers related to a given topic, including abstracts\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Defining Agents \n",
                "\n",
                "Next, we will define the agents that will perform the tasks. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
                "\n",
                "google_search_agent = AssistantAgent(\n",
                "    name=\"Google_Search_Agent\",\n",
                "    tools=[google_search_tool],\n",
                "    model_client=model_client,\n",
                "    description=\"An agent that can search Google for information, returns results with a snippet and body content\",\n",
                "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools.\",\n",
                ")\n",
                "\n",
                "arxiv_search_agent = AssistantAgent(\n",
                "    name=\"Arxiv_Search_Agent\",\n",
                "    tools=[arxiv_search_tool],\n",
                "    model_client=model_client,\n",
                "    description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts\",\n",
                "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools. Specifically, you can take into consideration the user's request and craft a search query that is most likely to return relevant academi papers.\",\n",
                ")\n",
                "\n",
                "\n",
                "report_agent = AssistantAgent(\n",
                "    name=\"Report_Agent\",\n",
                "    model_client=model_client,\n",
                "    description=\"Generate a report based on a given topic\",\n",
                "    system_message=\"You are a helpful assistant. Your task is to synthesize data extracted into a high quality literature review including CORRECT references. You MUST write a final report that is formatted as a literature review with CORRECT references.  Your response should end with the word 'TERMINATE'\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating the Team \n",
                "\n",
                "Finally, we will create a team of agents and configure them to perform the tasks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "termination = TextMentionTermination(\"TERMINATE\")\n",
                "team = RoundRobinGroupChat(\n",
                "    participants=[google_search_agent, arxiv_search_agent, report_agent], termination_condition=termination\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "await Console(\n",
                "    team.run_stream(\n",
                "        task=\"Write a literature review on no code tools for building multi agent ai systems\",\n",
                "    )\n",
                ")\n",
                "\n",
                "await model_client.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
